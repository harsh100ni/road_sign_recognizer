{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ee500c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split dataset: 613 training, 132 validation, 132 test images\n",
      "Class distribution saved to 'class_distribution.csv' and 'class_distribution.png'\n",
      "Data organized into train/val/test directories\n",
      "Found 4 classes: ['crosswalk' 'speedlimit' 'stop' 'trafficlight']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\00har\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:99: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">62</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">62</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">31</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">31</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">29</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">29</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4608</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">589,952</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">516</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m62\u001b[0m, \u001b[38;5;34m62\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │           \u001b[38;5;34m896\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d (\u001b[38;5;33mMaxPooling2D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m31\u001b[0m, \u001b[38;5;34m31\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m29\u001b[0m, \u001b[38;5;34m29\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │        \u001b[38;5;34m18,496\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_1 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │        \u001b[38;5;34m73,856\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_2 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4608\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │       \u001b[38;5;34m589,952\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m)              │           \u001b[38;5;34m516\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">683,716</span> (2.61 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m683,716\u001b[0m (2.61 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">683,716</span> (2.61 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m683,716\u001b[0m (2.61 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n",
      "Epoch 1/11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\00har\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:120: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.5565 - loss: 1.0717\n",
      "Epoch 1: val_accuracy improved from -inf to 0.76440, saving model to best_road_sign_model.keras\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 91ms/step - accuracy: 0.5594 - loss: 1.0663 - val_accuracy: 0.7644 - val_loss: 0.6540 - learning_rate: 0.0010\n",
      "Epoch 2/11\n",
      "\u001b[1m26/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.7934 - loss: 0.5519\n",
      "Epoch 2: val_accuracy improved from 0.76440 to 0.92147, saving model to best_road_sign_model.keras\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 91ms/step - accuracy: 0.7962 - loss: 0.5459 - val_accuracy: 0.9215 - val_loss: 0.2150 - learning_rate: 0.0010\n",
      "Epoch 3/11\n",
      "\u001b[1m26/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - accuracy: 0.9187 - loss: 0.2952\n",
      "Epoch 3: val_accuracy improved from 0.92147 to 0.95812, saving model to best_road_sign_model.keras\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 80ms/step - accuracy: 0.9196 - loss: 0.2919 - val_accuracy: 0.9581 - val_loss: 0.1145 - learning_rate: 0.0010\n",
      "Epoch 4/11\n",
      "\u001b[1m26/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.9327 - loss: 0.1793\n",
      "Epoch 4: val_accuracy improved from 0.95812 to 0.96859, saving model to best_road_sign_model.keras\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 78ms/step - accuracy: 0.9334 - loss: 0.1785 - val_accuracy: 0.9686 - val_loss: 0.0879 - learning_rate: 0.0010\n",
      "Epoch 5/11\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9550 - loss: 0.1351\n",
      "Epoch 5: val_accuracy improved from 0.96859 to 0.97382, saving model to best_road_sign_model.keras\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 76ms/step - accuracy: 0.9551 - loss: 0.1348 - val_accuracy: 0.9738 - val_loss: 0.0782 - learning_rate: 0.0010\n",
      "Epoch 6/11\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.9503 - loss: 0.1483\n",
      "Epoch 6: val_accuracy did not improve from 0.97382\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 73ms/step - accuracy: 0.9502 - loss: 0.1487 - val_accuracy: 0.9476 - val_loss: 0.1185 - learning_rate: 0.0010\n",
      "Epoch 7/11\n",
      "\u001b[1m26/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - accuracy: 0.9746 - loss: 0.0940\n",
      "Epoch 7: val_accuracy improved from 0.97382 to 0.98953, saving model to best_road_sign_model.keras\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 80ms/step - accuracy: 0.9739 - loss: 0.0954 - val_accuracy: 0.9895 - val_loss: 0.0479 - learning_rate: 0.0010\n",
      "Epoch 8/11\n",
      "\u001b[1m26/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - accuracy: 0.9715 - loss: 0.1131\n",
      "Epoch 8: val_accuracy did not improve from 0.98953\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 78ms/step - accuracy: 0.9716 - loss: 0.1121 - val_accuracy: 0.9791 - val_loss: 0.0437 - learning_rate: 0.0010\n",
      "Epoch 9/11\n",
      "\u001b[1m26/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - accuracy: 0.9695 - loss: 0.0954\n",
      "Epoch 9: val_accuracy did not improve from 0.98953\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 80ms/step - accuracy: 0.9699 - loss: 0.0951 - val_accuracy: 0.9686 - val_loss: 0.0758 - learning_rate: 0.0010\n",
      "Epoch 10/11\n",
      "\u001b[1m26/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.9772 - loss: 0.0620\n",
      "Epoch 10: val_accuracy did not improve from 0.98953\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 67ms/step - accuracy: 0.9772 - loss: 0.0620 - val_accuracy: 0.9686 - val_loss: 0.0645 - learning_rate: 0.0010\n",
      "Epoch 11/11\n",
      "\u001b[1m26/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - accuracy: 0.9869 - loss: 0.0533\n",
      "Epoch 11: val_accuracy did not improve from 0.98953\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 77ms/step - accuracy: 0.9873 - loss: 0.0525 - val_accuracy: 0.9686 - val_loss: 0.0850 - learning_rate: 0.0010\n",
      "Model saved as 'road_sign_model.keras'\n",
      "Evaluating model on test data...\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9961 - loss: 0.0350\n",
      "Test accuracy: 0.9897\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   crosswalk       1.00      1.00      1.00        28\n",
      "  speedlimit       0.98      1.00      0.99       116\n",
      "        stop       1.00      1.00      1.00        14\n",
      "trafficlight       1.00      0.94      0.97        36\n",
      "\n",
      "    accuracy                           0.99       194\n",
      "   macro avg       1.00      0.99      0.99       194\n",
      "weighted avg       0.99      0.99      0.99       194\n",
      "\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "Sample predictions visualization saved as 'sample_predictions.png'\n",
      "\n",
      "Measuring inference time...\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "Batch size 1: 0.0783s (12.77 images/s)\n",
      "Batch size 4: 0.0785s (50.97 images/s)\n",
      "Batch size 8: 0.0819s (97.66 images/s)\n",
      "Batch size 16: 0.0845s (189.28 images/s)\n",
      "Batch size 32: 0.0832s (384.84 images/s)\n",
      "Test results saved to 'test_results.txt'\n",
      "All visualizations and analysis complete!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "import numpy as np\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import time\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc\n",
    "from itertools import cycle\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "# Define paths to your data folders\n",
    "data_dir = \"signs1\"  # Change this to your data directory\n",
    "image_dir = os.path.join(data_dir, \"images\")\n",
    "annotation_dir = os.path.join(data_dir, \"annotations\")\n",
    "\n",
    "# Create directories for train/val/test split\n",
    "def create_split_directories():\n",
    "    split_dirs = {\n",
    "        \"train\": os.path.join(data_dir, \"train\"),\n",
    "        \"val\": os.path.join(data_dir, \"val\"),\n",
    "        \"test\": os.path.join(data_dir, \"test\")\n",
    "    }\n",
    "    \n",
    "    for split_name, split_path in split_dirs.items():\n",
    "        # Create main directory\n",
    "        os.makedirs(split_path, exist_ok=True)\n",
    "        \n",
    "        # Create subdirectories for images and annotations\n",
    "        os.makedirs(os.path.join(split_path, \"images\"), exist_ok=True)\n",
    "        os.makedirs(os.path.join(split_path, \"annotations\"), exist_ok=True)\n",
    "    \n",
    "    return split_dirs\n",
    "\n",
    "def parse_annotation(annotation_path):\n",
    "    \"\"\"Parse XML annotation file to extract object class and bounding box.\"\"\"\n",
    "    tree = ET.parse(annotation_path)\n",
    "    root = tree.getroot()\n",
    "    \n",
    "    objects = []\n",
    "    for obj in root.findall('./object'):\n",
    "        name = obj.find('name').text\n",
    "        bbox = obj.find('bndbox')\n",
    "        xmin = int(bbox.find('xmin').text)\n",
    "        ymin = int(bbox.find('ymin').text)\n",
    "        xmax = int(bbox.find('xmax').text)\n",
    "        ymax = int(bbox.find('ymax').text)\n",
    "        objects.append({\n",
    "            'class': name,\n",
    "            'bbox': [xmin, ymin, xmax, ymax]\n",
    "        })\n",
    "    \n",
    "    return objects\n",
    "\n",
    "def split_dataset(train_ratio=0.7, val_ratio=0.15, test_ratio=0.15):\n",
    "    \"\"\"Split the dataset into train, validation and test sets.\"\"\"\n",
    "    # Verify ratios\n",
    "    assert train_ratio + val_ratio + test_ratio == 1.0, \"Ratios must sum to 1\"\n",
    "    \n",
    "    # Get image files with matching annotations\n",
    "    image_files = []\n",
    "    image_classes = []\n",
    "    \n",
    "    for img_file in os.listdir(image_dir):\n",
    "        if not img_file.endswith('.png'):\n",
    "            continue\n",
    "        \n",
    "        # Check if annotation exists\n",
    "        annotation_file = os.path.splitext(img_file)[0] + '.xml'\n",
    "        annotation_path = os.path.join(annotation_dir, annotation_file)\n",
    "        \n",
    "        if os.path.exists(annotation_path):\n",
    "            image_files.append(img_file)\n",
    "            \n",
    "            # Get classes for stratification\n",
    "            objects = parse_annotation(annotation_path)\n",
    "            # Use the first object's class for stratification\n",
    "            if objects:\n",
    "                image_classes.append(objects[0]['class'])\n",
    "            else:\n",
    "                image_classes.append(\"unknown\")\n",
    "    \n",
    "    # First split: separate train+val from test\n",
    "    train_val_files, test_files = train_test_split(\n",
    "        image_files,\n",
    "        test_size=test_ratio,\n",
    "        random_state=42,\n",
    "        stratify=image_classes\n",
    "    )\n",
    "    \n",
    "    # Update classes for second split\n",
    "    train_val_classes = [image_classes[image_files.index(file)] for file in train_val_files]\n",
    "    \n",
    "    # Second split: separate train from val\n",
    "    train_files, val_files = train_test_split(\n",
    "        train_val_files,\n",
    "        test_size=val_ratio/(train_ratio+val_ratio),\n",
    "        random_state=42,\n",
    "        stratify=train_val_classes\n",
    "    )\n",
    "    \n",
    "    print(f\"Split dataset: {len(train_files)} training, {len(val_files)} validation, {len(test_files)} test images\")\n",
    "    \n",
    "    # Calculate class distribution for each split\n",
    "    class_distribution = calculate_class_distribution(train_files, val_files, test_files)\n",
    "    \n",
    "    return train_files, val_files, test_files, class_distribution\n",
    "\n",
    "def calculate_class_distribution(train_files, val_files, test_files):\n",
    "    \"\"\"Calculate the distribution of classes in each split.\"\"\"\n",
    "    splits = {\n",
    "        \"train\": train_files,\n",
    "        \"val\": val_files,\n",
    "        \"test\": test_files\n",
    "    }\n",
    "    \n",
    "    distributions = {}\n",
    "    \n",
    "    for split_name, files in splits.items():\n",
    "        class_counts = {}\n",
    "        \n",
    "        for img_file in files:\n",
    "            # Get annotation file\n",
    "            annotation_file = os.path.splitext(img_file)[0] + '.xml'\n",
    "            annotation_path = os.path.join(annotation_dir, annotation_file)\n",
    "            \n",
    "            if os.path.exists(annotation_path):\n",
    "                objects = parse_annotation(annotation_path)\n",
    "                \n",
    "                # Count each class in this image\n",
    "                for obj in objects:\n",
    "                    class_name = obj['class']\n",
    "                    if class_name in class_counts:\n",
    "                        class_counts[class_name] += 1\n",
    "                    else:\n",
    "                        class_counts[class_name] = 1\n",
    "        \n",
    "        distributions[split_name] = class_counts\n",
    "    \n",
    "    return distributions\n",
    "\n",
    "def visualize_class_distribution(class_distribution):\n",
    "    \"\"\"Generate Table B1: Distribution of classes in training, validation, and test sets.\"\"\"\n",
    "    # Convert the dictionary to a DataFrame for easier visualization\n",
    "    all_classes = set()\n",
    "    for split in class_distribution.values():\n",
    "        all_classes.update(split.keys())\n",
    "    \n",
    "    # Create empty DataFrame with all classes\n",
    "    df = pd.DataFrame(index=sorted(all_classes), columns=[\"Train\", \"Val\", \"Test\", \"Total\"])\n",
    "    \n",
    "    # Fill in the data\n",
    "    for class_name in all_classes:\n",
    "        df.loc[class_name, \"Train\"] = class_distribution[\"train\"].get(class_name, 0)\n",
    "        df.loc[class_name, \"Val\"] = class_distribution[\"val\"].get(class_name, 0)\n",
    "        df.loc[class_name, \"Test\"] = class_distribution[\"test\"].get(class_name, 0)\n",
    "        df.loc[class_name, \"Total\"] = (df.loc[class_name, \"Train\"] + \n",
    "                                    df.loc[class_name, \"Val\"] + \n",
    "                                    df.loc[class_name, \"Test\"])\n",
    "    \n",
    "    # Add totals row\n",
    "    df.loc[\"Total\", :] = df.sum()\n",
    "    \n",
    "    # Calculate percentages\n",
    "    total_images = df.loc[\"Total\", \"Total\"]\n",
    "    df[\"Train %\"] = (df[\"Train\"] / df[\"Total\"] * 100).round(1)\n",
    "    df[\"Val %\"] = (df[\"Val\"] / df[\"Total\"] * 100).round(1)\n",
    "    df[\"Test %\"] = (df[\"Test\"] / df[\"Total\"] * 100).round(1)\n",
    "    \n",
    "    # Save as CSV\n",
    "    df.to_csv(\"class_distribution.csv\")\n",
    "    \n",
    "    # Create a visual representation\n",
    "    plt.figure(figsize=(14, 10))\n",
    "    \n",
    "    # Get classes excluding the \"Total\" row\n",
    "    classes = df.index[:-1]\n",
    "    \n",
    "    # Set up bar positions\n",
    "    width = 0.25\n",
    "    x = np.arange(len(classes))\n",
    "    \n",
    "    # Create grouped bars\n",
    "    plt.bar(x - width, df.loc[classes, \"Train\"], width, label=\"Train\")\n",
    "    plt.bar(x, df.loc[classes, \"Val\"], width, label=\"Val\")\n",
    "    plt.bar(x + width, df.loc[classes, \"Test\"], width, label=\"Test\")\n",
    "    \n",
    "    # Customize plot\n",
    "    plt.xlabel(\"Sign Classes\")\n",
    "    plt.ylabel(\"Number of Instances\")\n",
    "    plt.title(\"Distribution of Classes in Train/Val/Test Sets\")\n",
    "    plt.xticks(x, classes, rotation=90)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the figure\n",
    "    plt.savefig(\"class_distribution.png\")\n",
    "    plt.close()\n",
    "    \n",
    "    print(\"Class distribution saved to 'class_distribution.csv' and 'class_distribution.png'\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def organize_data_split(split_dirs, train_files, val_files, test_files):\n",
    "    \"\"\"Copy files to their respective directories.\"\"\"\n",
    "    # Map of split names to file lists\n",
    "    splits = {\n",
    "        \"train\": train_files,\n",
    "        \"val\": val_files,\n",
    "        \"test\": test_files\n",
    "    }\n",
    "    \n",
    "    for split_name, files in splits.items():\n",
    "        split_img_dir = os.path.join(split_dirs[split_name], \"images\")\n",
    "        split_ann_dir = os.path.join(split_dirs[split_name], \"annotations\")\n",
    "        \n",
    "        for img_file in files:\n",
    "            # Copy image\n",
    "            src_img = os.path.join(image_dir, img_file)\n",
    "            dst_img = os.path.join(split_img_dir, img_file)\n",
    "            shutil.copy2(src_img, dst_img)\n",
    "            \n",
    "            # Copy annotation\n",
    "            ann_file = os.path.splitext(img_file)[0] + '.xml'\n",
    "            src_ann = os.path.join(annotation_dir, ann_file)\n",
    "            dst_ann = os.path.join(split_ann_dir, ann_file)\n",
    "            shutil.copy2(src_ann, dst_ann)\n",
    "    \n",
    "    print(\"Data organized into train/val/test directories\")\n",
    "\n",
    "def load_processed_dataset(image_dir, annotation_dir):\n",
    "    \"\"\"Load and preprocess images and annotations from a directory.\"\"\"\n",
    "    images = []\n",
    "    classes = []\n",
    "    filenames = []\n",
    "    \n",
    "    for img_file in os.listdir(image_dir):\n",
    "        if not img_file.endswith('.png'):\n",
    "            continue\n",
    "        \n",
    "        # Get corresponding annotation file\n",
    "        annotation_file = os.path.splitext(img_file)[0] + '.xml'\n",
    "        annotation_path = os.path.join(annotation_dir, annotation_file)\n",
    "        \n",
    "        if not os.path.exists(annotation_path):\n",
    "            print(f\"Warning: No annotation found for {img_file}\")\n",
    "            continue\n",
    "        \n",
    "        # Load and preprocess image\n",
    "        img_path = os.path.join(image_dir, img_file)\n",
    "        img = cv2.imread(img_path)\n",
    "        if img is None:\n",
    "            print(f\"Warning: Could not read image {img_path}\")\n",
    "            continue\n",
    "            \n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Parse annotation\n",
    "        objects = parse_annotation(annotation_path)\n",
    "        \n",
    "        for obj in objects:\n",
    "            # Extract the road sign using bounding box\n",
    "            xmin, ymin, xmax, ymax = obj['bbox']\n",
    "            \n",
    "            # Check if bbox is within image bounds\n",
    "            h, w = img.shape[:2]\n",
    "            xmin, ymin = max(0, xmin), max(0, ymin)\n",
    "            xmax, ymax = min(w, xmax), min(h, ymax)\n",
    "            \n",
    "            # Skip invalid boxes\n",
    "            if xmin >= xmax or ymin >= ymax:\n",
    "                print(f\"Warning: Invalid bbox in {img_file}\")\n",
    "                continue\n",
    "                \n",
    "            sign = img[ymin:ymax, xmin:xmax]\n",
    "            \n",
    "            # Resize to consistent dimensions\n",
    "            try:\n",
    "                sign = cv2.resize(sign, (64, 64))\n",
    "                \n",
    "                # Normalize pixel values\n",
    "                sign = sign / 255.0\n",
    "                \n",
    "                images.append(sign)\n",
    "                classes.append(obj['class'])\n",
    "                filenames.append(img_file)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {img_file}: {e}\")\n",
    "    \n",
    "    return np.array(images), np.array(classes), np.array(filenames)\n",
    "\n",
    "def create_data_generators(split_dirs):\n",
    "    \"\"\"Create data generators for each split.\"\"\"\n",
    "    # Load train data\n",
    "    train_images, train_classes, _ = load_processed_dataset(\n",
    "        os.path.join(split_dirs[\"train\"], \"images\"),\n",
    "        os.path.join(split_dirs[\"train\"], \"annotations\")\n",
    "    )\n",
    "    \n",
    "    # Load validation data\n",
    "    val_images, val_classes, _ = load_processed_dataset(\n",
    "        os.path.join(split_dirs[\"val\"], \"images\"),\n",
    "        os.path.join(split_dirs[\"val\"], \"annotations\")\n",
    "    )\n",
    "    \n",
    "    # Load test data\n",
    "    test_images, test_classes, test_filenames = load_processed_dataset(\n",
    "        os.path.join(split_dirs[\"test\"], \"images\"),\n",
    "        os.path.join(split_dirs[\"test\"], \"annotations\")\n",
    "    )\n",
    "    \n",
    "    # Check if we have data\n",
    "    if len(train_images) == 0:\n",
    "        raise ValueError(\"No training data found or processed\")\n",
    "    if len(val_images) == 0:\n",
    "        raise ValueError(\"No validation data found or processed\")\n",
    "    if len(test_images) == 0:\n",
    "        raise ValueError(\"No test data found or processed\")\n",
    "    \n",
    "    # Encode labels\n",
    "    label_encoder = LabelEncoder()\n",
    "    label_encoder.fit(np.concatenate([train_classes, val_classes, test_classes]))\n",
    "    \n",
    "    train_encoded = label_encoder.transform(train_classes)\n",
    "    val_encoded = label_encoder.transform(val_classes)\n",
    "    test_encoded = label_encoder.transform(test_classes)\n",
    "    \n",
    "    num_classes = len(label_encoder.classes_)\n",
    "    print(f\"Found {num_classes} classes: {label_encoder.classes_}\")\n",
    "    \n",
    "    # Create data generators\n",
    "    train_gen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "        rotation_range=15,\n",
    "        width_shift_range=0.1,\n",
    "        height_shift_range=0.1,\n",
    "        zoom_range=0.1,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest'\n",
    "    ).flow(train_images, train_encoded, batch_size=32)\n",
    "    \n",
    "    # No augmentation for validation and test sets\n",
    "    val_data = (val_images, val_encoded)\n",
    "    test_data = (test_images, test_encoded)\n",
    "    \n",
    "    return train_gen, val_data, test_data, label_encoder, test_filenames\n",
    "\n",
    "def create_model(num_classes):\n",
    "    \"\"\"Create a CNN model for road sign classification.\"\"\"\n",
    "    model = models.Sequential()\n",
    "    \n",
    "    # Convolutional layers\n",
    "    model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(64, 64, 3)))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    \n",
    "    # Fully connected layers\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(128, activation='relu'))\n",
    "    model.add(layers.Dropout(0.5))\n",
    "    model.add(layers.Dense(num_classes, activation='softmax'))\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam',\n",
    "                loss='sparse_categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def train_model(model, train_gen, val_data, epochs=11):\n",
    "    \"\"\"Train the model with the given data generators.\"\"\"\n",
    "    # Use early stopping to prevent overfitting\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss', patience=5, restore_best_weights=True\n",
    "    )\n",
    "    \n",
    "    # Checkpoint to save best model - FIXED: use .keras extension\n",
    "    model_checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath='best_road_sign_model.keras',\n",
    "        monitor='val_accuracy',\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Reduce learning rate when plateauing\n",
    "    reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.2,\n",
    "        patience=3,\n",
    "        min_lr=1e-6\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    try:\n",
    "        history = model.fit(\n",
    "            train_gen,\n",
    "            epochs=epochs,\n",
    "            validation_data=val_data,\n",
    "            callbacks=[early_stopping, model_checkpoint, reduce_lr]\n",
    "        )\n",
    "        \n",
    "        # [Figure 1: Training and validation accuracy/loss curves]\n",
    "        plt.figure(figsize=(12, 10))\n",
    "        \n",
    "        # Plot training & validation accuracy\n",
    "        plt.subplot(2, 1, 1)\n",
    "        plt.plot(history.history['accuracy'], marker='o', linestyle='-', label='Train Accuracy')\n",
    "        plt.plot(history.history['val_accuracy'], marker='s', linestyle='--', label='Validation Accuracy')\n",
    "        plt.grid(True, linestyle='--', alpha=0.7)\n",
    "        plt.xlabel('Epoch', fontsize=12)\n",
    "        plt.ylabel('Accuracy', fontsize=12)\n",
    "        plt.title('Training and Validation Accuracy', fontsize=14)\n",
    "        plt.legend(loc='lower right', fontsize=12)\n",
    "        \n",
    "        # Plot training & validation loss\n",
    "        plt.subplot(2, 1, 2)\n",
    "        plt.plot(history.history['loss'], marker='o', linestyle='-', label='Train Loss', color='tab:orange')\n",
    "        plt.plot(history.history['val_loss'], marker='s', linestyle='--', label='Validation Loss', color='tab:green')\n",
    "        plt.grid(True, linestyle='--', alpha=0.7)\n",
    "        plt.xlabel('Epoch', fontsize=12)\n",
    "        plt.ylabel('Loss', fontsize=12)\n",
    "        plt.title('Training and Validation Loss', fontsize=14)\n",
    "        plt.legend(loc='upper right', fontsize=12)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('training_history.png', dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        # Additional display of learning rate if it changed during training\n",
    "        if 'lr' in history.history:\n",
    "            plt.figure(figsize=(10, 4))\n",
    "            plt.plot(history.history['lr'], marker='o')\n",
    "            plt.grid(True, linestyle='--', alpha=0.7)\n",
    "            plt.xlabel('Epoch', fontsize=12)\n",
    "            plt.ylabel('Learning Rate', fontsize=12)\n",
    "            plt.title('Learning Rate Schedule', fontsize=14)\n",
    "            plt.yscale('log')\n",
    "            plt.savefig('learning_rate_schedule.png', dpi=300, bbox_inches='tight')\n",
    "            plt.close()\n",
    "        \n",
    "        return history\n",
    "    except Exception as e:\n",
    "        print(f\"Error during training: {e}\")\n",
    "        return None\n",
    "\n",
    "def evaluate_model(model, test_data, label_encoder, test_filenames):\n",
    "    \"\"\"Evaluate model on test data and visualize results.\"\"\"\n",
    "    # Unpack test data\n",
    "    test_images, test_labels = test_data\n",
    "    \n",
    "    # Evaluate overall performance\n",
    "    test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
    "    print(f\"Test accuracy: {test_acc:.4f}\")\n",
    "    \n",
    "    # Make predictions\n",
    "    predictions = model.predict(test_images)\n",
    "    predicted_classes = np.argmax(predictions, axis=1)\n",
    "    predicted_labels = label_encoder.inverse_transform(predicted_classes)\n",
    "    true_labels = label_encoder.inverse_transform(test_labels)\n",
    "    \n",
    "    # Create confusion matrix\n",
    "    cm = confusion_matrix(test_labels, predicted_classes)\n",
    "    \n",
    "    # [Figure 2: Visualization of confusion matrix]\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    \n",
    "    # Plot normalized confusion matrix\n",
    "    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    sns.heatmap(cm_normalized, annot=True, fmt='.2f', cmap='Blues',\n",
    "            xticklabels=label_encoder.classes_,\n",
    "            yticklabels=label_encoder.classes_)\n",
    "    plt.title('Normalized Confusion Matrix', fontsize=16)\n",
    "    plt.xlabel('Predicted Label', fontsize=14)\n",
    "    plt.ylabel('True Label', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # [Table C1: Extended classification report with additional metrics]\n",
    "    # Calculate per-class metrics\n",
    "    class_metrics = {}\n",
    "    \n",
    "    # Calculate metrics for each class\n",
    "    for i, class_name in enumerate(label_encoder.classes_):\n",
    "        # True positives, false positives, false negatives\n",
    "        tp = cm[i, i]\n",
    "        fp = np.sum(cm[:, i]) - tp\n",
    "        fn = np.sum(cm[i, :]) - tp\n",
    "        tn = np.sum(cm) - tp - fp - fn\n",
    "        \n",
    "        # Calculate metrics\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        \n",
    "        class_metrics[class_name] = {\n",
    "            'Precision': precision,\n",
    "            'Recall': recall,\n",
    "            'F1-score': f1,\n",
    "            'Specificity': specificity,\n",
    "            'Support': np.sum(test_labels == i)\n",
    "        }\n",
    "    \n",
    "    # Create DataFrame for extended metrics\n",
    "    extended_metrics_df = pd.DataFrame.from_dict(class_metrics, orient='index')\n",
    "    extended_metrics_df.to_csv('extended_classification_metrics.csv')\n",
    "    \n",
    "    # Print classification report\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(test_labels, predicted_classes, target_names=label_encoder.classes_))\n",
    "    \n",
    "    # [Figure C1: ROC curves for multi-class classification]\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    \n",
    "    # Compute ROC curve and ROC area for each class\n",
    "    n_classes = len(label_encoder.classes_)\n",
    "    \n",
    "    # Binarize the output for ROC calculation\n",
    "    y_test_bin = tf.keras.utils.to_categorical(test_labels, n_classes)\n",
    "    \n",
    "    # Variables to store per-class performance\n",
    "    fpr = {}\n",
    "    tpr = {}\n",
    "    roc_auc = {}\n",
    "    \n",
    "    # Calculate ROC for each class\n",
    "    for i in range(n_classes):\n",
    "        fpr[i], tpr[i], _ = roc_curve(y_test_bin[:, i], predictions[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "    \n",
    "    # Calculate macro-average ROC curve\n",
    "    all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
    "    mean_tpr = np.zeros_like(all_fpr)\n",
    "    \n",
    "    for i in range(n_classes):\n",
    "        # Fix: Use numpy's interp instead of scipy's interp\n",
    "        if len(fpr[i]) > 1:  # Make sure we have points to interpolate\n",
    "            interpolator = interp1d(fpr[i], tpr[i], kind='linear', bounds_error=False, fill_value=(0, 1))\n",
    "            mean_tpr += interpolator(all_fpr)\n",
    "        else:\n",
    "            # If only one point, can't interpolate, just add zeros\n",
    "            mean_tpr += 0\n",
    "    \n",
    "    mean_tpr /= n_classes\n",
    "    \n",
    "    # Plot ROC curves\n",
    "    colors = plt.colormaps['tab10'].resampled(n_classes)\n",
    "    \n",
    "    # Plot individual ROC curves if there are fewer than 10 classes, otherwise just plot the average\n",
    "    if n_classes <= 10:\n",
    "        for i, color in zip(range(n_classes), colors.colors):\n",
    "            plt.plot(fpr[i], tpr[i], color=color, lw=2,\n",
    "                    label=f'{label_encoder.classes_[i]} (AUC = {roc_auc[i]:.2f})')\n",
    "    \n",
    "    # Plot macro-average ROC curve\n",
    "    plt.plot(all_fpr, mean_tpr, color='deeppink', linestyle=':', linewidth=4,\n",
    "            label=f'Macro-average (AUC = {auc(all_fpr, mean_tpr):.2f})')\n",
    "    \n",
    "    # Plot chance level\n",
    "    plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "    \n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate', fontsize=14)\n",
    "    plt.ylabel('True Positive Rate', fontsize=14)\n",
    "    plt.title('Multi-class ROC Curves', fontsize=16)\n",
    "    plt.legend(loc=\"lower right\", fontsize=10)\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.savefig('roc_curves.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # Visualize some predictions\n",
    "    num_samples_to_show = min(10, len(test_images))\n",
    "    fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i in range(num_samples_to_show):\n",
    "        axes[i].imshow(test_images[i])\n",
    "        true_class = true_labels[i]\n",
    "        pred_class = predicted_labels[i]\n",
    "        confidence = predictions[i][predicted_classes[i]]\n",
    "        \n",
    "        color = \"green\" if true_class == pred_class else \"red\"\n",
    "        axes[i].set_title(f\"True: {true_class}\\nPred: {pred_class}\\nConf: {confidence:.2f}\", color=color)\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('prediction_samples.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Return prediction results\n",
    "    return {\n",
    "        'accuracy': test_acc,\n",
    "        'predictions': [\n",
    "            {\n",
    "                'filename': test_filenames[i],\n",
    "                'true_class': true_labels[i],\n",
    "                'predicted_class': predicted_labels[i],\n",
    "                'confidence': predictions[i][predicted_classes[i]]\n",
    "            }\n",
    "            for i in range(len(test_images))\n",
    "        ]\n",
    "    }\n",
    "\n",
    "def analyze_inference_time(model, test_images, hardware_platforms=None):\n",
    "    \"\"\"\n",
    "    [Table C2: Inference time analysis on different hardware platforms]\n",
    "    Analyze inference time on the current platform and simulate others.\n",
    "    \"\"\"\n",
    "    if hardware_platforms is None:\n",
    "        # Define hardware platforms to simulate\n",
    "        hardware_platforms = {\n",
    "            \"Current Platform\": 1.0,  # Baseline\n",
    "            \"CPU (Lower Spec)\": 2.5,  # Simulated: 2.5x slower\n",
    "            \"CPU (Higher Spec)\": 0.6,  # Simulated: 40% faster\n",
    "            \"GPU (Entry Level)\": 0.3,  # Simulated: 70% faster\n",
    "            \"GPU (High End)\": 0.1,    # Simulated: 90% faster\n",
    "            \"TPU/Specialized\": 0.05    # Simulated: 95% faster\n",
    "        }\n",
    "    \n",
    "    # Measure baseline inference time\n",
    "    print(\"\\nMeasuring inference time...\")\n",
    "    \n",
    "    # Warm-up run\n",
    "    _ = model.predict(test_images[:10])\n",
    "    \n",
    "    # Measure time with different batch sizes\n",
    "    batch_sizes = [1, 4, 8, 16, 32]\n",
    "    results = []\n",
    "    \n",
    "    for batch_size in batch_sizes:\n",
    "        times = []\n",
    "        \n",
    "        # Measure batch inference times\n",
    "        num_batches = max(1, min(10, len(test_images) // batch_size))\n",
    "        \n",
    "        for i in range(num_batches):\n",
    "            start_idx = i * batch_size\n",
    "            end_idx = min((i + 1) * batch_size, len(test_images))\n",
    "            batch = test_images[start_idx:end_idx]\n",
    "            \n",
    "            # Ensure batch is correctly sized\n",
    "            if len(batch) != batch_size:\n",
    "                continue\n",
    "            \n",
    "            # Time the inference\n",
    "            start_time = time.time()\n",
    "            _ = model.predict(batch, verbose=0)\n",
    "            end_time = time.time()\n",
    "            \n",
    "            times.append(end_time - start_time)\n",
    "        \n",
    "        # Calculate average time\n",
    "        if times:\n",
    "            avg_time = np.mean(times)\n",
    "            per_image_time = avg_time / batch_size\n",
    "            fps = batch_size / avg_time\n",
    "            \n",
    "            print(f\"Batch size {batch_size}: {avg_time:.4f}s ({fps:.2f} images/s)\")\n",
    "            \n",
    "            # Add to results\n",
    "            batch_result = {\n",
    "                'Batch Size': batch_size,\n",
    "                'Avg Inference Time (s)': avg_time,\n",
    "                'Per Image Time (s)': per_image_time,\n",
    "                'FPS': fps\n",
    "            }\n",
    "            \n",
    "            # Simulate other hardware platforms\n",
    "            for platform, factor in hardware_platforms.items():\n",
    "                if platform != \"Current Platform\":\n",
    "                    simulated_time = avg_time * factor\n",
    "                    simulated_per_image = per_image_time * factor\n",
    "                    simulated_fps = fps / factor\n",
    "                    \n",
    "                    batch_result[f'{platform} Time (s)'] = simulated_time\n",
    "                    batch_result[f'{platform} Per Image (s)'] = simulated_per_image\n",
    "                    batch_result[f'{platform} FPS'] = simulated_fps\n",
    "            \n",
    "            results.append(batch_result)\n",
    "    \n",
    "    # Create DataFrame and save results\n",
    "    inference_df = pd.DataFrame(results)\n",
    "    inference_df.to_csv('inference_time_analysis.csv')\n",
    "    \n",
    "    # Create visualization of inference time vs batch size\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    \n",
    "    # Plot per-image inference time\n",
    "    plt.subplot(1, 2, 1)\n",
    "    for platform in hardware_platforms.keys():\n",
    "        if platform == \"Current Platform\":\n",
    "            plt.plot(inference_df['Batch Size'], inference_df['Per Image Time (s)'], \n",
    "                    marker='o', linewidth=2, label=platform)\n",
    "        else:\n",
    "            column = f'{platform} Per Image (s)'\n",
    "            plt.plot(inference_df['Batch Size'], inference_df[column], \n",
    "                    marker='s', linewidth=2, linestyle='--', label=platform)\n",
    "    \n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.xlabel('Batch Size', fontsize=12)\n",
    "    plt.ylabel('Per Image Inference Time (s)', fontsize=12)\n",
    "    plt.title('Inference Time by Batch Size', fontsize=14)\n",
    "    plt.legend()\n",
    "    plt.xscale('log', base=2)\n",
    "    plt.yscale('log')\n",
    "    \n",
    "    # Plot FPS\n",
    "    plt.subplot(1, 2, 2)\n",
    "    for platform in hardware_platforms.keys():\n",
    "        if platform == \"Current Platform\":\n",
    "            plt.plot(inference_df['Batch Size'], inference_df['FPS'], \n",
    "                    marker='o', linewidth=2, label=platform)\n",
    "        else:\n",
    "            column = f'{platform} FPS'\n",
    "            plt.plot(inference_df['Batch Size'], inference_df[column], \n",
    "                    marker='s', linewidth=2, linestyle='--', label=platform)\n",
    "    \n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.xlabel('Batch Size', fontsize=12)\n",
    "    plt.ylabel('Frames Per Second (FPS)', fontsize=12)\n",
    "    plt.title('Throughput by Batch Size', fontsize=14)\n",
    "    plt.legend()\n",
    "    plt.xscale('log', base=2)\n",
    "    plt.yscale('log')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('inference_time_analysis.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    return inference_df\n",
    "\n",
    "def visualize_sample_predictions(model, test_data, label_encoder, test_filenames, num_samples=20):\n",
    "    \"\"\"\n",
    "    [Figure: Sample test images with predictions]\n",
    "    Visualize a grid of test images with their true and predicted labels.\n",
    "    \"\"\"\n",
    "    # Unpack test data\n",
    "    test_images, test_labels = test_data\n",
    "    \n",
    "    # Make predictions\n",
    "    predictions = model.predict(test_images)\n",
    "    predicted_classes = np.argmax(predictions, axis=1)\n",
    "    predicted_labels = label_encoder.inverse_transform(predicted_classes)\n",
    "    true_labels = label_encoder.inverse_transform(test_labels)\n",
    "    \n",
    "    # Determine number of samples to show (minimum of requested or available)\n",
    "    num_samples_to_show = min(num_samples, len(test_images))\n",
    "    \n",
    "    # Calculate grid dimensions\n",
    "    grid_size = int(np.ceil(np.sqrt(num_samples_to_show)))\n",
    "    fig, axes = plt.subplots(grid_size, grid_size, figsize=(15, 15))\n",
    "    \n",
    "    # Flatten axes for easier indexing\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    # Select a mix of correct and incorrect predictions\n",
    "    correct_indices = np.where(predicted_classes == test_labels)[0]\n",
    "    incorrect_indices = np.where(predicted_classes != test_labels)[0]\n",
    "    \n",
    "    # Ensure we have some of each if possible\n",
    "    selected_indices = []\n",
    "    if len(correct_indices) > 0 and len(incorrect_indices) > 0:\n",
    "        # Try to get half correct and half incorrect\n",
    "        num_correct = min(num_samples_to_show // 2, len(correct_indices))\n",
    "        num_incorrect = min(num_samples_to_show - num_correct, len(incorrect_indices))\n",
    "        \n",
    "        selected_indices = np.random.choice(correct_indices, num_correct, replace=False).tolist()\n",
    "        selected_indices += np.random.choice(incorrect_indices, num_incorrect, replace=False).tolist()\n",
    "    else:\n",
    "        # If we have only correct or only incorrect, just use what we have\n",
    "        available_indices = np.arange(len(test_images))\n",
    "        selected_indices = np.random.choice(available_indices, num_samples_to_show, replace=False)\n",
    "    \n",
    "    # Ensure we don't exceed the number of samples to show\n",
    "    selected_indices = selected_indices[:num_samples_to_show]\n",
    "    \n",
    "    # Visualize selected samples\n",
    "    for i, idx in enumerate(selected_indices):\n",
    "        if i < len(axes):\n",
    "            axes[i].imshow(test_images[idx])\n",
    "            true_class = true_labels[idx]\n",
    "            pred_class = predicted_labels[idx]\n",
    "            confidence = predictions[idx][predicted_classes[idx]]\n",
    "            \n",
    "            # Set title color based on prediction correctness\n",
    "            color = \"green\" if true_class == pred_class else \"red\"\n",
    "            \n",
    "            # Create informative title\n",
    "            title = f\"True: {true_class}\\nPred: {pred_class}\\nConf: {confidence:.2f}\"\n",
    "            \n",
    "            # Add filename if available\n",
    "            if test_filenames is not None and idx < len(test_filenames):\n",
    "                title += f\"\\n{test_filenames[idx]}\"\n",
    "                \n",
    "            axes[i].set_title(title, color=color, fontsize=10)\n",
    "            axes[i].axis('off')\n",
    "    \n",
    "    # Hide any unused subplots\n",
    "    for i in range(len(selected_indices), len(axes)):\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('sample_predictions.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"Sample predictions visualization saved as 'sample_predictions.png'\")\n",
    "\n",
    "def main():\n",
    "    # Create split directories\n",
    "    split_dirs = create_split_directories()\n",
    "    \n",
    "    # Split dataset\n",
    "    train_files, val_files, test_files, class_distribution = split_dataset()\n",
    "    \n",
    "    # [Table B1: Distribution of classes in training, validation, and test sets]\n",
    "    distribution_df = visualize_class_distribution(class_distribution)\n",
    "    \n",
    "    # Organize data into splits\n",
    "    organize_data_split(split_dirs, train_files, val_files, test_files)\n",
    "    \n",
    "    # Create data generators\n",
    "    train_gen, val_data, test_data, label_encoder, test_filenames = create_data_generators(split_dirs)\n",
    "    \n",
    "    # Create model\n",
    "    num_classes = len(label_encoder.classes_)\n",
    "    model = create_model(num_classes)\n",
    "    \n",
    "    # Print model summary\n",
    "    model.summary()\n",
    "    \n",
    "    # Train model\n",
    "    print(\"Training model...\")\n",
    "    history = train_model(model, train_gen, val_data)\n",
    "    \n",
    "    # Save model and label encoder\n",
    "    model.save(\"road_sign_model.keras\")\n",
    "    np.save(\"label_classes.npy\", label_encoder.classes_)\n",
    "    print(\"Model saved as 'road_sign_model.keras'\")\n",
    "    \n",
    "    # Evaluate on test data\n",
    "    print(\"Evaluating model on test data...\")\n",
    "    results = evaluate_model(model, test_data, label_encoder, test_filenames)\n",
    "    \n",
    "    visualize_sample_predictions(model, test_data, label_encoder, test_filenames)\n",
    "    \n",
    "    # Analyze inference time on different simulated hardware platforms\n",
    "    inference_analysis = analyze_inference_time(model, test_data[0])\n",
    "    \n",
    "    # Save results to file\n",
    "    with open(\"test_results.txt\", \"w\") as f:\n",
    "        f.write(f\"Test Accuracy: {results['accuracy']:.4f}\\n\\n\")\n",
    "        f.write(\"Individual Predictions:\\n\")\n",
    "        for pred in results['predictions']:\n",
    "            f.write(f\"File: {pred['filename']}\\n\")\n",
    "            f.write(f\"True Class: {pred['true_class']}\\n\")\n",
    "            f.write(f\"Predicted Class: {pred['predicted_class']}\\n\")\n",
    "            f.write(f\"Confidence: {pred['confidence']:.4f}\\n\")\n",
    "            f.write(\"-\" * 40 + \"\\n\")\n",
    "    \n",
    "    print(f\"Test results saved to 'test_results.txt'\")\n",
    "    print(\"All visualizations and analysis complete!\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2725b467",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--input INPUT] [--output OUTPUT]\n",
      "                             [--model MODEL] [--config CONFIG]\n",
      "                             [--classes CLASSES] [--confidence CONFIDENCE]\n",
      "                             [--no-display]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: --f=c:\\Users\\00har\\AppData\\Roaming\\jupyter\\runtime\\kernel-v3f8b632730f8cf6b6c1bac3b724224b8eb975c639.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\00har\\AppData\\Roaming\\Python\\Python312\\site-packages\\IPython\\core\\interactiveshell.py:3585: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "class DrivingDetectionSystem:\n",
    "    def __init__(self, detection_model=\"yolov3.weights\", \n",
    "                detection_config=\"yolov3.cfg\",\n",
    "                classes_file=\"coco.names\",\n",
    "                traffic_sign_model=None,\n",
    "                confidence_threshold=0.5):\n",
    "        \"\"\"\n",
    "        Initialize the driving detection system\n",
    "        \n",
    "        Parameters:\n",
    "        - detection_model: Path to YOLOv3 weights file\n",
    "        - detection_config: Path to YOLOv3 config file\n",
    "        - classes_file: Path to class names file\n",
    "        - traffic_sign_model: Path to traffic sign classifier (if available)\n",
    "        - confidence_threshold: Minimum confidence for detections\n",
    "        \"\"\"\n",
    "        self.confidence_threshold = confidence_threshold\n",
    "        self.nms_threshold = 0.4  # Non-maximum suppression threshold\n",
    "        \n",
    "        # Load YOLO network for general object detection\n",
    "        print(\"Loading YOLO object detection model...\")\n",
    "        try:\n",
    "            self.net = cv2.dnn.readNetFromDarknet(detection_config, detection_model)\n",
    "            self.net.setPreferableBackend(cv2.dnn.DNN_BACKEND_OPENCV)\n",
    "            # Use CPU by default, but can be changed to CUDA for better performance\n",
    "            # if GPU is available\n",
    "            self.net.setPreferableTarget(cv2.dnn.DNN_TARGET_CPU)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading detection model: {str(e)}\")\n",
    "            print(\"Using simulation mode - model weights not required for demo.\")\n",
    "            self.net = None\n",
    "            \n",
    "        # Load class names\n",
    "        try:\n",
    "            with open(classes_file, 'r') as f:\n",
    "                self.classes = [line.strip() for line in f.readlines()]\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading class names: {str(e)}\")\n",
    "            # Fallback to common class names for driving scenarios\n",
    "            self.classes = ['person', 'bicycle', 'car', 'motorcycle', 'bus', \n",
    "                        'truck', 'traffic light', 'stop sign', 'parking meter']\n",
    "            \n",
    "        self.traffic_sign_model = traffic_sign_model\n",
    "        self.colors = np.random.uniform(0, 255, size=(len(self.classes), 3))\n",
    "        \n",
    "        # Define which objects are especially relevant for driving\n",
    "        self.critical_objects = ['person', 'bicycle', 'car', 'motorcycle', 'bus', \n",
    "                                'truck', 'traffic light', 'stop sign']\n",
    "                                \n",
    "        # Initialize video writer for saving detections\n",
    "        self.output_writer = None\n",
    "    \n",
    "    def _get_output_layers(self):\n",
    "        \"\"\"Get the output layer names of the YOLO network\"\"\"\n",
    "        if self.net is None:\n",
    "            return []\n",
    "            \n",
    "        layer_names = self.net.getLayerNames()\n",
    "        try:\n",
    "            # Different versions of OpenCV have different indexing\n",
    "            try:\n",
    "                output_layers = [layer_names[i - 1] for i in self.net.getUnconnectedOutLayers()]\n",
    "            except:\n",
    "                output_layers = [layer_names[i[0] - 1] for i in self.net.getUnconnectedOutLayers()]\n",
    "            return output_layers\n",
    "        except Exception as e:\n",
    "            print(f\"Error getting output layers: {str(e)}\")\n",
    "            return []\n",
    "    \n",
    "    def _simulate_detections(self, frame):\n",
    "        \"\"\"\n",
    "        Simulate detections for demo purposes when no model is available\n",
    "        \"\"\"\n",
    "        height, width = frame.shape[:2]\n",
    "        boxes = []\n",
    "        confidences = []\n",
    "        class_ids = []\n",
    "        \n",
    "        # Simulate a car detection\n",
    "        if np.random.random() > 0.3:\n",
    "            car_x = int(width * (0.5 + 0.1 * np.sin(time.time())))\n",
    "            car_y = int(height * 0.7)\n",
    "            car_w = int(width * 0.2)\n",
    "            car_h = int(height * 0.2)\n",
    "            boxes.append([car_x, car_y, car_w, car_h])\n",
    "            confidences.append(0.85)\n",
    "            class_ids.append(2)  # Car\n",
    "        \n",
    "        # Simulate a person detection occasionally\n",
    "        if np.random.random() > 0.7:\n",
    "            person_x = int(width * (0.7 + 0.05 * np.cos(time.time())))\n",
    "            person_y = int(height * 0.6)\n",
    "            person_w = int(width * 0.05)\n",
    "            person_h = int(height * 0.2)\n",
    "            boxes.append([person_x, person_y, person_w, person_h])\n",
    "            confidences.append(0.75)\n",
    "            class_ids.append(0)  # Person\n",
    "            \n",
    "        # Simulate a traffic sign or light occasionally\n",
    "        if np.random.random() > 0.8:\n",
    "            sign_x = int(width * 0.8)\n",
    "            sign_y = int(height * 0.4)\n",
    "            sign_w = int(width * 0.06)\n",
    "            sign_h = int(height * 0.06)\n",
    "            boxes.append([sign_x, sign_y, sign_w, sign_h])\n",
    "            confidences.append(0.65)\n",
    "            class_ids.append(9 if np.random.random() > 0.5 else 7)  # Traffic light or stop sign\n",
    "            \n",
    "        return boxes, confidences, class_ids\n",
    "    \n",
    "    def detect_objects(self, frame):\n",
    "        \"\"\"\n",
    "        Detect objects in a frame using YOLOv3\n",
    "        \n",
    "        Parameters:\n",
    "        - frame: Input video frame\n",
    "        \n",
    "        Returns:\n",
    "        - boxes: Bounding boxes of detected objects\n",
    "        - confidences: Confidence scores\n",
    "        - class_ids: Class IDs of detected objects\n",
    "        \"\"\"\n",
    "        if self.net is None:\n",
    "            # Simulation mode if model not loaded\n",
    "            return self._simulate_detections(frame)\n",
    "            \n",
    "        height, width = frame.shape[:2]\n",
    "        \n",
    "        # Create a blob from the frame and perform a forward pass\n",
    "        blob = cv2.dnn.blobFromImage(frame, 1/255.0, (416, 416), \n",
    "                                    swapRB=True, crop=False)\n",
    "        self.net.setInput(blob)\n",
    "        \n",
    "        outputs = self.net.forward(self._get_output_layers())\n",
    "        \n",
    "        # Process the outputs\n",
    "        boxes = []\n",
    "        confidences = []\n",
    "        class_ids = []\n",
    "        \n",
    "        for output in outputs:\n",
    "            for detection in output:\n",
    "                scores = detection[5:]\n",
    "                class_id = np.argmax(scores)\n",
    "                confidence = scores[class_id]\n",
    "                \n",
    "                if confidence > self.confidence_threshold:\n",
    "                    # Scale the bounding box coordinates back to the original frame\n",
    "                    box_x = int(detection[0] * width)\n",
    "                    box_y = int(detection[1] * height)\n",
    "                    box_width = int(detection[2] * width)\n",
    "                    box_height = int(detection[3] * height)\n",
    "                    \n",
    "                    # Store the detection results\n",
    "                    boxes.append([box_x, box_y, box_width, box_height])\n",
    "                    confidences.append(float(confidence))\n",
    "                    class_ids.append(class_id)\n",
    "        \n",
    "        return boxes, confidences, class_ids\n",
    "    \n",
    "    def detect_traffic_signs(self, frame, boxes):\n",
    "        \"\"\"\n",
    "        Detect specific traffic signs using a dedicated classifier\n",
    "        (This is a placeholder - would be implemented with a specific model)\n",
    "        \n",
    "        Parameters:\n",
    "        - frame: Input video frame\n",
    "        - boxes: Detected bounding boxes from general object detection\n",
    "        \n",
    "        Returns:\n",
    "        - sign_types: List of detected sign types\n",
    "        \"\"\"\n",
    "        # This would be implemented with a dedicated traffic sign classifier\n",
    "        # For now, it's just a placeholder\n",
    "        sign_types = []\n",
    "        if self.traffic_sign_model:\n",
    "            # Logic to classify traffic signs would go here\n",
    "            pass\n",
    "        return sign_types\n",
    "    \n",
    "    def process_frame(self, frame):\n",
    "        \"\"\"\n",
    "        Process a single frame for object and sign detection\n",
    "        \n",
    "        Parameters:\n",
    "        - frame: Input video frame\n",
    "        \n",
    "        Returns:\n",
    "        - processed_frame: Frame with detections drawn\n",
    "        - detections: Dictionary of detection information\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Detect objects\n",
    "        boxes, confidences, class_ids = self.detect_objects(frame)\n",
    "        \n",
    "        # Apply non-maximum suppression to remove overlapping bounding boxes\n",
    "        indices = cv2.dnn.NMSBoxes(boxes, confidences, \n",
    "                                self.confidence_threshold, \n",
    "                                self.nms_threshold)\n",
    "        \n",
    "        # Prepare the processed frame and detections dictionary\n",
    "        processed_frame = frame.copy()\n",
    "        detections = {\n",
    "            'objects': [],\n",
    "            'critical_warnings': []\n",
    "        }\n",
    "        \n",
    "        # Draw bounding boxes and labels\n",
    "        if len(indices) > 0:\n",
    "            for i in indices.flatten():\n",
    "                try:\n",
    "                    box = boxes[i]\n",
    "                    x, y, w, h = box\n",
    "                    \n",
    "                    # Ensure coordinates are within frame boundaries\n",
    "                    x = max(0, min(x, frame.shape[1] - 1))\n",
    "                    y = max(0, min(y, frame.shape[0] - 1))\n",
    "                    w = max(1, min(w, frame.shape[1] - x))\n",
    "                    h = max(1, min(h, frame.shape[0] - y))\n",
    "                    \n",
    "                    # Get class information\n",
    "                    class_id = class_ids[i]\n",
    "                    if class_id < len(self.classes):\n",
    "                        label = f\"{self.classes[class_id]}: {confidences[i]:.2f}\"\n",
    "                        color = self.colors[class_id]\n",
    "                        \n",
    "                        # Draw bounding box\n",
    "                        cv2.rectangle(processed_frame, (x, y), (x + w, y + h), color, 2)\n",
    "                        \n",
    "                        # Draw label background\n",
    "                        cv2.rectangle(processed_frame, (x, y - 20), (x + len(label) * 9, y), color, -1)\n",
    "                        \n",
    "                        # Draw label text\n",
    "                        cv2.putText(processed_frame, label, (x, y - 5),\n",
    "                                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 1)\n",
    "                        \n",
    "                        # Add to detections list\n",
    "                        detections['objects'].append({\n",
    "                            'class': self.classes[class_id],\n",
    "                            'confidence': confidences[i],\n",
    "                            'box': [x, y, w, h]\n",
    "                        })\n",
    "                        \n",
    "                        # Check if this is a critical object for driving\n",
    "                        if self.classes[class_id] in self.critical_objects:\n",
    "                            # Calculate the position in the frame (center, left, right)\n",
    "                            center_x = x + w/2\n",
    "                            frame_center = frame.shape[1] / 2\n",
    "                            position = \"ahead\"\n",
    "                            if center_x < frame_center * 0.7:\n",
    "                                position = \"left\"\n",
    "                            elif center_x > frame_center * 1.3:\n",
    "                                position = \"right\"\n",
    "                                \n",
    "                            # Add critical warning based on object type and position\n",
    "                            if self.classes[class_id] == 'stop sign':\n",
    "                                detections['critical_warnings'].append(f\"STOP SIGN {position}\")\n",
    "                            elif self.classes[class_id] == 'traffic light':\n",
    "                                detections['critical_warnings'].append(f\"TRAFFIC LIGHT {position}\")\n",
    "                            elif self.classes[class_id] in ['person', 'bicycle']:\n",
    "                                # Person or cyclist is closer if the box is larger\n",
    "                                if (w * h) > (frame.shape[0] * frame.shape[1] * 0.05):\n",
    "                                    detections['critical_warnings'].append(\n",
    "                                        f\"WARNING: {self.classes[class_id].upper()} {position}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing detection {i}: {str(e)}\")\n",
    "        \n",
    "        # Calculate and display FPS\n",
    "        fps = 1.0 / (time.time() - start_time)\n",
    "        cv2.putText(processed_frame, f\"FPS: {fps:.2f}\", (10, 30),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "        \n",
    "        # Display critical warnings\n",
    "        for i, warning in enumerate(detections['critical_warnings']):\n",
    "            cv2.putText(processed_frame, warning, (10, 70 + 40*i),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "        \n",
    "        return processed_frame, detections\n",
    "    \n",
    "    def process_video(self, input_source, output_path=None, display=True):\n",
    "        \"\"\"\n",
    "        Process a video source (file or camera) for object and sign detection\n",
    "        \n",
    "        Parameters:\n",
    "        - input_source: Path to video file or camera index\n",
    "        - output_path: Path to save processed video (optional)\n",
    "        - display: Whether to display the processed frames\n",
    "        \"\"\"\n",
    "        # Open the video source\n",
    "        video = cv2.VideoCapture(input_source)\n",
    "        if not video.isOpened():\n",
    "            print(f\"Error: Could not open video source {input_source}\")\n",
    "            return\n",
    "        \n",
    "        # Get video properties\n",
    "        width = int(video.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "        height = int(video.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "        fps = video.get(cv2.CAP_PROP_FPS)\n",
    "        \n",
    "        # Initialize video writer if output path is given\n",
    "        if output_path:\n",
    "            fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "            self.output_writer = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "        \n",
    "        print(\"Starting video processing. Press 'q' to quit.\")\n",
    "        \n",
    "        while True:\n",
    "            # Read a frame\n",
    "            ret, frame = video.read()\n",
    "            if not ret:\n",
    "                print(\"End of video or error reading frame.\")\n",
    "                break\n",
    "            \n",
    "            # Process the frame\n",
    "            processed_frame, detections = self.process_frame(frame)\n",
    "            \n",
    "            # Display the frame if requested\n",
    "            if display:\n",
    "                cv2.imshow(\"Driving Detection System\", processed_frame)\n",
    "                \n",
    "                # Break on 'q' key press\n",
    "                if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                    break\n",
    "            \n",
    "            # Write the frame to output video if requested\n",
    "            if self.output_writer:\n",
    "                self.output_writer.write(processed_frame)\n",
    "        \n",
    "        # Clean up\n",
    "        video.release()\n",
    "        if self.output_writer:\n",
    "            self.output_writer.release()\n",
    "        cv2.destroyAllWindows()\n",
    "        print(\"Video processing complete.\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to run the driving detection system\n",
    "    \"\"\"\n",
    "    import argparse\n",
    "    \n",
    "    parser = argparse.ArgumentParser(description='Driving Object and Sign Detection')\n",
    "    parser.add_argument('--input', type=str, default='0',\n",
    "                        help='Input video file path or camera index (default: 0 for webcam)')\n",
    "    parser.add_argument('--output', type=str, default=None,\n",
    "                        help='Output video file path (optional)')\n",
    "    parser.add_argument('--model', type=str, default='yolov3.weights',\n",
    "                        help='Path to YOLO weights file')\n",
    "    parser.add_argument('--config', type=str, default='yolov3.cfg',\n",
    "                        help='Path to YOLO config file')\n",
    "    parser.add_argument('--classes', type=str, default='coco.names',\n",
    "                        help='Path to class names file')\n",
    "    parser.add_argument('--confidence', type=float, default=0.5,\n",
    "                        help='Minimum confidence threshold for detections')\n",
    "    parser.add_argument('--no-display', action='store_true',\n",
    "                        help='Do not display video output')\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    # Initialize the detection system\n",
    "    detector = DrivingDetectionSystem(\n",
    "        detection_model=args.model,\n",
    "        detection_config=args.config,\n",
    "        classes_file=args.classes,\n",
    "        confidence_threshold=args.confidence\n",
    "    )\n",
    "    \n",
    "    # Process the video\n",
    "    try:\n",
    "        # If input is a number, convert to integer for camera index\n",
    "        input_source = args.input\n",
    "        if input_source.isdigit():\n",
    "            input_source = int(input_source)\n",
    "        \n",
    "        detector.process_video(\n",
    "            input_source=input_source,\n",
    "            output_path=args.output,\n",
    "            display=not args.no_display\n",
    "        )\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Processing interrupted by user.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing video: {str(e)}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
